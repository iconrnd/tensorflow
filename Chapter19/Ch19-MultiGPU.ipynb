{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03301a7-06e1-44b7-977d-76339a8a1bfc",
   "metadata": {},
   "source": [
    "## Multi GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab5c41-a8ec-404e-8ea3-504783d977c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b8e49-3a9a-4723-9f85-be064deff6f5",
   "metadata": {},
   "source": [
    "## Listing Physical Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0df25-3942-4251-91ef-0f3dfd52ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devies = tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7aa1b-8c48-4d58-8ea1-9dccd3ada63d",
   "metadata": {},
   "source": [
    "## Limiting GPU visibility \n",
    "\n",
    "### Set env vars:\n",
    "* CUDA_DEVICE_ORDER=PCI_BUS_ID \n",
    "* CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237cdf0-a4c0-4b94-b1f7-97a267f1327e",
   "metadata": {},
   "source": [
    "## Limiting RAM usage per GPU with logical GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f9146-a570-421d-85ee-0015cbeec8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in physical_devies:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpu,\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974e2fd-4ae4-4c3c-b04a-8d4d729c4b53",
   "metadata": {},
   "source": [
    "## Memory use growth\n",
    "\n",
    "* Set env var: TF_FORCE_GPU_ALLOW_GROWTH = True\n",
    "\n",
    "### or in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9a5eb-a614-4019-a224-bfbc7c57e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in physical_devies:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392d276-12ee-455e-9fb3-09cf810728dc",
   "metadata": {},
   "source": [
    "## Splitting GPU into multiple logical GPUs for e.g. distributed training tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e62f7-888e-4170-a2f6-52535db5c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_logical_device_configuration(\n",
    "        physical_devies[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=2048), # -> \"/gpu:0\"\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=2048)] # -> \"/gpu:1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26356ab8-c63b-42be-b7f0-abfbc79c155e",
   "metadata": {},
   "source": [
    "## Variables placement in devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0b920-5b38-4025-97d5-8ddadcc3a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642c974f-b48c-4bf0-9194-85162bc782c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386dc35-0c7b-4b4b-82aa-64d26ab5a096",
   "metadata": {},
   "source": [
    "## No kernel for int32 ops so var placed on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86eb5f4-7251-418e-811c-259dda48476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e065f5a0-dbc2-4670-850a-6b19d7b9517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763de5a-e0a8-469c-b05c-1871de6b98d3",
   "metadata": {},
   "source": [
    "## tf.int8 and tf.int16 have GPU kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7b9b5e-734f-4035-886d-0471c5ab4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = tf.Variable([1, 2, 3], dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b621057a-adbd-4560-895f-a63cbc2fa612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7b2fe-8720-44a5-aece-91517675148f",
   "metadata": {},
   "source": [
    "## Foriceing var placement with dev context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403fda10-14e9-4a73-a7a7-c7dfd3aaaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    c = tf.Variable(1., 2., 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766b0fda-c94c-4688-85c5-20d6d9180bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff586f8f-aba1-4e19-9b44-1c4ee671674b",
   "metadata": {},
   "source": [
    "## Disabling device placement fallback with an explicit exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a960b-6934-4495-955d-59c97bbdbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_soft_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c840c-7e5a-401e-95a7-9abf727c1f76",
   "metadata": {},
   "source": [
    "## Control over intra and inter-op treads pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b2ef2-128f-48be-b3d7-d60b2f74ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.threading.set_inter_op_parallelism_threads()\n",
    "tf.config.threading.set_intra_op_parallelism_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921b3d9-6107-43d4-91af-b100617275b7",
   "metadata": {},
   "source": [
    "## Prefetching to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd80cb-2ae4-4ac8-9c55-faace910650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.prefetch_to_device() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a9c30-87d1-4b96-9f61-7306ab06db52",
   "metadata": {},
   "source": [
    "## Model parallelism: MESH TF\n",
    "* https://github.com/tensorflow/mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91642b-0959-4d90-9f87-9eba0678554c",
   "metadata": {},
   "source": [
    "## Data parallelism - strategies operating on different minibatches\n",
    "\n",
    "* Mirrored Strategy: Identical replicas, AllReduce for gradient mean and update sync\n",
    "* Centralized parameters server and GPU workers, allows for async updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a59a6-fcb2-4d3d-88e3-fd5524a8f0d9",
   "metadata": {},
   "source": [
    "## Bandwidth saturation limits the number of GPUs useful in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bce86-46f1-4a54-94c5-9770602fb903",
   "metadata": {},
   "source": [
    "## Strategy with more central parameters server can reduce server strain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b181cfb-ba08-4fb6-b1d7-840716fd926c",
   "metadata": {},
   "source": [
    "## Massive parallelism: PipeDream and more recent Pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd4b73-42e0-4d54-8bc5-8808fbbd906a",
   "metadata": {},
   "source": [
    "## Reducing data load by reducing model pecision from tf.float32 to tf.float 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e0f02-80ae-48a0-84a1-b36d34bdf5c3",
   "metadata": {},
   "source": [
    "## Distribution strategies API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877bcb4-b911-4905-8d20-b7d92dbf9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f907a12-cd41-4a24-85cd-4b0082fdfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([...])\n",
    "    model.compile([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561a37f-aa62-47f3-b984-0180a100ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crucually: batch_size should be divisible \n",
    "# by the number of replicas so that each batch\n",
    "# would have the same size\n",
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10,\n",
    "    validation_data=(X_valid, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a5f70-4279-4d45-a8bb-fecd31948bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.weights[0]) # -> tensorflow.python.distribute.values.MirroredVariabl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb000db-0a75-4d87-9ba4-7c56659449c2",
   "metadata": {},
   "source": [
    "## Running saved model on multiple GPUs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f27c9e-8060-4359-bc88-f50d7199ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"my_mirrored_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73149c-9294-47e5-b6e3-389d3f50a723",
   "metadata": {},
   "source": [
    "## Specifyibg which GPUs to use with strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55ad16-8536-4952-a7e7-f48b77c1c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b4612-8555-47ac-8088-c8b1bd5999cb",
   "metadata": {},
   "source": [
    "## Other reduction strategies: set *cross_device_ops* to\n",
    "* tf.distribute.HierarchicalCopyAllReduce\n",
    "* tf.distribute.ReductionToOneDevice\n",
    "* Tge default NCCL is tf.distribute.NcclAllReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15310fae-0dcd-44fe-8ae0-6f5b8a8f28dc",
   "metadata": {},
   "source": [
    "## Data parallelism with parameters server\n",
    "* strategy = tf.distribute.experimental.CentralStorageStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2d74e-e80c-4e0f-b17e-008b514347c9",
   "metadata": {},
   "source": [
    "## TF Cluster: a group of TF processes working in parallel (also can be distributed)\n",
    "* Each TF process in the cluster is a *task* or *TF Server*\n",
    "* Has IP, port, type (role/job): worker, chief, ps, evaluator, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdfb8b-aa1d-4eb1-9bd0-b665e41f0103",
   "metadata": {},
   "source": [
    "## Cluster spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be5def-f947-4fb9-aefe-974972233d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\", # /job:worker/task:0\n",
    "        \"machine-b.example.com:2222\"  # /job:worker/task:1\n",
    "    ],\n",
    "    \"ps\": [\"machine-a.example.com:2221\"] # /job:ps/task:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5c8a1-9846-4664-a7f9-c6566f1294a9",
   "metadata": {},
   "source": [
    "## Data specifying the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa33022-ce81-4050-9274-9fc215d84c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be placed outside Python code so the same code\n",
    "# can be used for all task servers\n",
    "\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    \"cluster\": cluster_spec,\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c7c47-6e2b-46c7-8792-55d4a6289fb8",
   "metadata": {},
   "source": [
    "## Triggering the task requires running the same script on all servers\n",
    "\n",
    "## With *MultiWorkerMirroredStrategy* all workers must perform the same steps to ensure proper sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52195541-2fdc-4f5e-b86e-49f5c539f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "\n",
    "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
    "\n",
    "[...] # Dataset preparation\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([...])\n",
    "    model.compile([...])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
    "\n",
    "if resolver.task_id == 0: # This node is the chief, it saves the model\n",
    "    model.save('my_mnist_multiworker_model', save_format='tf')\n",
    "else:\n",
    "    # Formal steps needed to keep all workers in sync with chief\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "    model.save(tmpdir, save_format='tf')\n",
    "    tf.io.gfile.rmtree(tmpdir) # Nothin is stored on workers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc7766-41de-4c8c-af67-9cd07187135b",
   "metadata": {},
   "source": [
    "## Changing AllReduce strategy: TF runs heuristics to select the best algorithm but explicit choice can be made as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1bda3-e58d-49f1-8c67-bbed60350f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224a5a3-7739-4037-9b5b-ad135b3195d4",
   "metadata": {},
   "source": [
    "## Async data parallelism is used with *ParameterServerStrategy*\n",
    "* Add one or more param servers\n",
    "* Configure TF_CONFIG properly for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f2b2f-61d5-4936-8b9f-916a1bff8dfd",
   "metadata": {},
   "source": [
    "## TPU strategy (run just after importing tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb74245-5fae-4b27-a90e-cd3bf0fce974",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6dabc-032a-4fa3-af59-a8d383631f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
