{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16eadf8-1f89-4d47-a10e-6d0b380b550c",
   "metadata": {},
   "source": [
    "## Transformer put together as a model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48688693-4ccf-492a-b93b-a704d4b7b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(K.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm_layer_wide=K.layers.Normalization()\n",
    "        self.norm_layer_deep=K.layers.Normalization()\n",
    "        self.hidden_layer1=K.layers.Dense(units, activation=activation)\n",
    "        self.hidden_layer2=K.layers.Dense(units, activation=activation)\n",
    "        self.main_output=K.layers.Dense(1, name='main_output')\n",
    "        self.aux_output=K.layers.Dense(1, name='aux_output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_wide=inputs[0]\n",
    "        input_deep=inputs[1]\n",
    "        norm_wide=self.norm_layer_wide(input_wide)\n",
    "        norm_deep=self.norm_layer_deep(input_deep)\n",
    "        hidden1=self.hidden_layer1(norm_deep)\n",
    "        hidden2=self.hidden_layer2(hidden1)\n",
    "        concat=K.layers.concatenate([norm_wide, hidden2])\n",
    "        \n",
    "        return {'main_output':self.main_output(concat), 'aux_output':self.aux_output(hidden2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a664977-5014-42b1-86bf-f252ae47bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=WideAndDeepModel(30, activation='relu', name='my_cool_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5f538-19a2-4b02-8721-93e8c6293637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Transformer(K.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embed_size, \n",
    "                 max_length,\n",
    "                 max_length = 50,\n",
    "                 N = 2,\n",
    "                 num_heads = 8,\n",
    "                 dropout_rate = 0.1,\n",
    "                 n_units = 128,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.max_length = max_length \n",
    "        self.N = N\n",
    "        self.max_length = max_length\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_units = n_units\n",
    "\n",
    "        # Encoder and decoder inputs\n",
    "        #self.encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "        #self.decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "        # Encoder and decoder tokenized inputs embedding in embed_size dimensional space\n",
    "        # Maskings zeros ignores contribution from padding zeros to the loss\n",
    "        self.encoder_embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True)\n",
    "        self.decoder_embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True)\n",
    "\n",
    "        self.pos_embed_layer = PositionalEncoding(self.max_length, self.embed_size)\n",
    "\n",
    "        self.attn_layer_enc = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads, key_dim=self.embed_size, dropout=self.dropout_rate)\n",
    "\n",
    "        self.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "        self.add_layer = tf.keras.layers.Add()\n",
    "\n",
    "        self.dense_enc_0 = tf.keras.layers.Dense(self.n_units, activation=\"relu\")\n",
    "        self.dense_enc_1 = tf.keras.layers.Dense(self.embed_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        self.attn_layer_dec_0 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_size, dropout=self.dropout_rate)\n",
    "        self.attn_layer_dec_1 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_size, dropout=self.dropout_rate)\n",
    "\n",
    "        self.dense_dec_0 = tf.keras.layers.Dense(self.n_units, activation=\"relu\")\n",
    "        self.dense_dec_1 = tf.keras.layers.Dense(self.embed_size)\n",
    "\n",
    "        self.dense_output = tf.keras.layers.Dense(self.vocab_size, activation=\"softmax\")\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            encoder_inputs = inputs[0]\n",
    "            decoder_inputs = inputs[1]\n",
    "\n",
    "            # Encoder and decoder inputs tokenization\n",
    "            # At this point tokenizers are already adapted above\n",
    "            encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "            decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "            # Casting to float32 for consistency\n",
    "            encoder_input_ids = tf.cast(encoder_input_ids, tf.float32)\n",
    "            decoder_input_ids = tf.cast(decoder_input_ids, tf.float32)\n",
    "\n",
    "            encoder_embeddings = self.encoder_embedding_layer(encoder_input_ids)\n",
    "            decoder_embeddings = self.decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "            batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "\n",
    "            encoder_in = self.pos_embed_layer(encoder_embeddings)\n",
    "            decoder_in = self.pos_embed_layer(decoder_embeddings)\n",
    "\n",
    "            encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "            \n",
    "            # Input data\n",
    "            Z = encoder_in\n",
    "\n",
    "            # Encoder block\n",
    "            for _ in range(self.N):\n",
    "                skip = Z\n",
    "    \n",
    "                Z = self.attn_layer_enc(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "                Z = self.norm_layer()(sef.add_layer([Z, skip]))\n",
    "                \n",
    "                skip = Z\n",
    "                Z = dense_enc_0(Z)\n",
    "                Z = dense_enc_1(Z)\n",
    "                Z = self.dropout(Z)\n",
    "                Z = self.norm_layer(self.add_layer([Z, skip]))\n",
    "\n",
    "            \n",
    "            decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "\n",
    "            causal_mask = tf.linalg.band_part(\n",
    "                tf.ones((self.batch_max_len_dec, self.batch_max_len_dec), tf.bool), -1, 0)\n",
    "\n",
    "            encoder_outputs = Z\n",
    "            Z = decoder_in\n",
    "\n",
    "            \n",
    "            for _ in range(self.N):\n",
    "                skip = Z\n",
    "    \n",
    "                Z = sef.attn_layer_dec_0(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
    "                Z = tf.norm_layer(self.add_layer([Z, skip]))\n",
    "                \n",
    "                skip = Z\n",
    "    \n",
    "                # Cross-Attenion: Query from decoder, Key and Value from Encoder\n",
    "                Z = self.attn_layer_dec_1(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "                Z = self.norm_layer(self.add_layer([Z, skip]))\n",
    "                \n",
    "                skip = Z\n",
    "                Z = self.dense_dec_0(Z)\n",
    "                Z = self.dense_dec_1(Z)\n",
    "                Z = self.norm_layer(self.add_layer([Z, skip]))\n",
    "\n",
    "            Y_proba = self.dense_output(Z)\n",
    "            #Y_proba._keras_mask = Y_proba._keras_mask[:, :, tf.newaxis]#, :, tf.newaxis]\n",
    "            return Y_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20acc44-fed5-473d-b406-7bc0e9f175c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350bf5c-000f-4815-a716-16601b8202e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d81141-16bd-46ec-895e-1de34b0e8c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13320c6f-e659-435e-a86d-f27456be82a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8ecdb-ba52-4c39-865f-305ce20506d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358c789-ecb4-47b0-a2fa-e92dab21c98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55928fc0-4ee0-40ac-bef2-9285661e6824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2da747-4219-46a9-ac77-e3247f43ec9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30218dde-b012-42e6-8e46-5d60439fdf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e50e72-af80-4cb9-9bbf-b52c11b0ede4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461f685-bcf4-4e60-9bcd-9460c7525a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69afec8-c20c-4268-a01f-cd603473efb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc63641-414b-4c50-8667-47b97efe066e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ef6ad-d043-419c-866c-00777efb3e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458400de-b41d-4de8-9a05-f860d0862401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab67e9-274a-495c-948d-21ab32a2e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f032cd3-799f-434e-815a-0c8d2c3ab4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
