{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26dc9c5-37b5-4658-9275-a05d3b99de8d",
   "metadata": {},
   "source": [
    "## NMT with Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a5a5b-7957-494b-ac5a-5406944a5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e082c42-012d-4616-8059-a9c1be115fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6591186b-3e5c-4d6f-9e33-a762eb576931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = (Path(path).with_name(\"spa-eng\")/\"spa.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c968f6-7d49-43af-8533-eec9683ee9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/.keras/datasets/spa-eng.zip')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af13e978-6cf3-49ed-93f3-9386055d2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('¡','').replace('¿','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceb5cd6-a0c5-400b-a089-a07e04f9fc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.\\tVe.\\nGo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402e4e5c-524d-46bd-95b1-ce7cff96f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580384.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roughly 1.5M words\n",
    "len(text)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d5e8be-fecb-482d-91f4-fd0e4c4575b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation pairs are separated by tabs\n",
    "pairs = [line.split('\\t') for line in text.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b870b17d-4cff-47a5-b0fe-0a380ba99b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go.', 'Ve.'],\n",
       " ['Go.', 'Vete.'],\n",
       " ['Go.', 'Vaya.'],\n",
       " ['Go.', 'Váyase.'],\n",
       " ['Hi.', 'Hola.'],\n",
       " ['Run!', 'Corre!'],\n",
       " ['Run.', 'Corred.'],\n",
       " ['Who?', 'Quién?'],\n",
       " ['Fire!', 'Fuego!'],\n",
       " ['Fire!', 'Incendio!']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a94d0a0-9930-40f2-a716-a238bfd626aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inplace shuffling the whole dataset\n",
    "np.random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc82be9-b78b-4dde-896e-92a5926206d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "416b396c-bb23-46ea-95eb-34f635c69005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding based on the first 1000 words\n",
    "vocab_size = 1000\n",
    "\n",
    "# Max sentence length counted in tokes, so whole words here\n",
    "max_length = 50\n",
    "\n",
    "# Ebedding dimension\n",
    "embed_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15e1d6-a6e7-4ea0-84c5-f84c243527d3",
   "metadata": {},
   "source": [
    "## Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a6d42-cdfe-46f1-9b68-45136269f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43000f44-69ad-4b06-86fc-a6eb4bad6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48618fd9-2787-43aa-89bc-06c393e46524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb105a02-abd9-440c-998b-6129f0204586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e274617-c162-4bbb-a7af-871ca224c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Tom held a knife to Mary's throat.\",)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_en[99999:100_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13790c5-8186-4db5-8be6-cc3c4335d833",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2134c74-b86f-40a5-9484-a5327a5448f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "\n",
    "# Data shifter by one token for teacher forcing\n",
    "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
    "\n",
    "# The last predicted token must indicate sentence end\n",
    "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5518560a-0391-4a8a-9f9a-7a122b093e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'startofseq Es \\xc3\\xa9l japon\\xc3\\xa9s?',\n",
       "       b'startofseq Tom le puso un cuchillo en la garganta a Mary.'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dec[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ba8234-e42f-4154-a208-841f516307bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.cast(Y_train, tf.float32)\n",
    "Y_valid = tf.cast(Y_valid, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac93ce58-975a-4bfc-8d9a-1841cfb92732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100000, 50), dtype=float32, numpy=\n",
       "array([[  1.,   6.,   9., ...,   0.,   0.,   0.],\n",
       "       [ 28.,  86.,   1., ...,   0.,   0.,   0.],\n",
       "       [ 16.,  25.,  28., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [ 16.,  25.,  28., ...,   0.,   0.,   0.],\n",
       "       [ 12.,  44., 593., ...,   0.,   0.,   0.],\n",
       "       [  8.,  26., 299., ...,   0.,   0.,   0.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c8f839-dca7-490a-85cf-ffbf6a45c47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18964, 50), dtype=float32, numpy=\n",
       "array([[ 27., 665.,  18., ...,   0.,   0.,   0.],\n",
       "       [ 37.,   1.,   3., ...,   0.,   0.,   0.],\n",
       "       [ 14.,  72., 940., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [ 25., 103.,   8., ...,   0.,   0.,   0.],\n",
       "       [ 20.,  15.,   1., ...,   0.,   0.,   0.],\n",
       "       [  1.,  43., 831., ...,   0.,   0.,   0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb98feb-9ab4-4710-a7af-118e1f550c95",
   "metadata": {},
   "source": [
    "## Embedding and tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3590a7-eba1-4b18-9e4e-76f61f7b6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder inputs\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0269cc09-5e51-4826-b468-cf3ccba8cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder inputs tokenization\n",
    "# At this point tokenizers are already adapted above\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "# Casting to float32 for consistency\n",
    "encoder_input_ids = tf.cast(encoder_input_ids, tf.float32)\n",
    "decoder_input_ids = tf.cast(decoder_input_ids, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5794e9af-3287-4b5a-8462-5f70e6b111ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder tokenized inputs embedding in embed_size dimensional space\n",
    "# Maskings zeros ignores contribution from padding zeros to the loss\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b70080f-67b4-452e-a7ae-c0510701ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcdcad36-82a1-4ebf-9a28-db58cd3044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6670fa-bb86-43aa-99d7-ed0c74189f7d",
   "metadata": {},
   "source": [
    "### Learnable positional encoding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c89fc8f-85d4-4ea5-bdbf-c2d62ff8a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
    "\n",
    "#batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "#encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "\n",
    "#batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "#decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d15d6-0eb5-4c82-9d52-5902460a15ef",
   "metadata": {},
   "source": [
    "## Fixed positional encoing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52525d1-ebbb-41a7-aa6f-28055581c144",
   "metadata": {},
   "source": [
    "### Meshgrid allows fast vectorized evaluations on grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afaa9ab9-5c1e-45f1-855b-d8f6bc1efade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [0, 1, 2, 3]]),\n",
       " array([[0.  , 0.  , 0.  , 0.  ],\n",
       "        [3.14, 3.14, 3.14, 3.14],\n",
       "        [6.28, 6.28, 6.28, 6.28]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = np.meshgrid(np.arange(4), 3.14 * np.arange(3))\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfb8956a-613c-4c88-9d6c-b2844ef58ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(2 ** aux[0] + 3 ** aux[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90df6a33-3758-48f6-9120-d8beb2c61a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(max_length),\n",
    "                           2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        # Here we use the same values for i as above, since for odd embedding posiitons\n",
    "        # we use (i-1) as the exponent value, which evaluates to the value i of the even case above\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "350b2773-a9ab-43e6-892e-131bf6fd78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1476b1a8-901c-466a-a814-3b375bf39a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PositionalEncoding at 0x7fde3c544d00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1766a1f9-bb7d-4848-915f-7345a27b8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "924a456e-cf72-4fd0-adf6-802d95ef2b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 50, 128) dtype=float32 (created by layer 'positional_encoding')>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baaf338b-82f8-4982-9f59-338fb57c0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_unit = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4172464c-cf57-4694-922f-1625b9e70661",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a668c28-1443-4aed-8209-22ad5c1ebf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 50) dtype=bool (created by layer 'tf.__operators__.getitem_1')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c95abcb2-ad3b-4bbe-baa1-93f6475301b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for the first encoder skip connection\n",
    "Z = encoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b07e70-0c0d-4f5f-bc98-d0025422278c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_unit, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c926da4a-dedd-4e17-b800-61a1a6a186b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 50, 128) dtype=float32 (created by layer 'layer_normalization_3')>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10444f36-7195-4d8a-9dc4-830ef8dd554e",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b49cef7-40a8-4fb9-9cc7-efc8a8b33acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "\n",
    "causal_mask = tf.linalg.band_part(\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f30a9d84-5de7-4d3c-bf53-c86fc0e681d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = Z\n",
    "Z = decoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e673d9b-f564-433a-aafb-a931c47cbd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 50, 128) dtype=float32 (created by layer 'positional_encoding')>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69fa3481-6556-440f-bbe1-ee632c736746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    # Cross-Attenion: Query from decoder, Key and Value from Encoder\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_unit, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f97f08cc-d9bc-448b-9f5c-3819e7da2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a987e-ce43-445f-9dbc-e21c4ace2967",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "908a0221-93b1-471d-af85-a43ffbeeda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7f19500-1c6a-4d3a-a473-072cdd185780",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"],)\n",
    "             #jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a8b8002-dee5-4d39-99e4-a49d18a3b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_1 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " text_vectorization (TextVe  (None, 50)                   0         ['input_1[0][0]']             \n",
      " ctorization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)      (None, 50)                   0         ['text_vectorization_1[0][0]']\n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 50)                   0         ['text_vectorization[0][0]']  \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 50, 128)              128000    ['tf.cast_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLam  (None, 50)                   0         ['tf.cast[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 50, 128)              128000    ['tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " positional_encoding (Posit  (None, 50, 128)              0         ['embedding[0][0]',           \n",
      " ionalEncoding)                                                      'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (None, 1, 50)                0         ['tf.math.not_equal[0][0]']   \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 50, 128)              527488    ['positional_encoding[0][0]', \n",
      " iHeadAttention)                                                     'tf.__operators__.getitem_1[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'positional_encoding[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 50, 128)              0         ['multi_head_attention[0][0]',\n",
      "                                                                     'positional_encoding[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 50, 128)              256       ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 50, 128)              16512     ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 50, 128)              16512     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 50, 128)              0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 50, 128)              0         ['dropout[0][0]',             \n",
      "                                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 50, 128)              256       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLa  (3,)                         0         ['embedding_1[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 50, 128)              527488    ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'tf.__operators__.getitem_1[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 50, 128)              0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.ones (TFOpLambda)        (50, 50)                     0         ['tf.__operators__.getitem[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.math.not_equal_1 (TFOpL  (None, 50)                   0         ['tf.cast_1[0][0]']           \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 50, 128)              256       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.linalg.band_part (TFOpL  (50, 50)                     0         ['tf.ones[0][0]']             \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  (None, 1, 50)                0         ['tf.math.not_equal_1[0][0]'] \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 50, 128)              16512     ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.math.logical_and (TFOpL  (None, 50, 50)               0         ['tf.linalg.band_part[0][0]', \n",
      " ambda)                                                              'tf.__operators__.getitem_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 50, 128)              16512     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 50, 128)              527488    ['positional_encoding[1][0]', \n",
      " ltiHeadAttention)                                                   'tf.math.logical_and[0][0]', \n",
      "                                                                     'positional_encoding[1][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 50, 128)              0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 50, 128)              0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'positional_encoding[1][0]'] \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 50, 128)              0         ['dropout_1[0][0]',           \n",
      "                                                                     'layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 50, 128)              256       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 50, 128)              256       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 50, 128)              527488    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'tf.__operators__.getitem_1[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 50, 128)              0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 50, 128)              256       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 50, 128)              16512     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 50, 128)              16512     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 50, 128)              0         ['dense_5[0][0]',             \n",
      "                                                                     'layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 50, 128)              256       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.logical_and_1 (TFO  (None, 50, 50)               0         ['tf.linalg.band_part[0][0]', \n",
      " pLambda)                                                            'tf.__operators__.getitem_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 50, 128)              527488    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'tf.math.logical_and_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 50, 128)              0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 50, 128)              256       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 50, 128)              527488    ['layer_normalization_7[0][0]'\n",
      " ltiHeadAttention)                                                  , 'tf.__operators__.getitem_1[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 50, 128)              0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 50, 128)              256       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 50, 128)              16512     ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 50, 128)              16512     ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 50, 128)              0         ['dense_7[0][0]',             \n",
      "                                                                     'layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 50, 128)              256       ['add_9[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 50, 1000)             129000    ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3684584 (14.06 MB)\n",
      "Trainable params: 3684584 (14.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dad6612-2a40-4f85-9b98-0f6cf0e04787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict((X_valid[:2], X_valid_dec[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11d1ea-8507-47bc-b0a7-e03e0f49d5ce",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984d786-2fbb-41fb-a5a2-fae8a2ef3a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit((X_train, X_train_dec), Y_train, \n",
    "          epochs=2, \n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15069a66-9519-4ac8-bfae-ba9ef1633d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])\n",
    "        X_dec = np.array([\"startofseq \" + translation])\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == 'endofseq':\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092c75c-9d21-4828-86d8-0c6c8879afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "689d5544-439f-4d80-840c-33464b17f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100000, 50])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "660e6415-97b9-4487-bc68-a38faafecbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[7.2149835, 6.326355 , 6.5161777, 7.1277547, 6.1781445, 7.4830623,\n",
       "        7.329293 , 6.813641 , 6.8545556, 6.78256  , 6.5896783, 6.4026375,\n",
       "        6.3167057, 6.3564377, 6.6299744, 6.8215413, 6.781695 , 6.657982 ,\n",
       "        6.5429077, 6.4739647, 6.7684364, 7.109344 , 7.1886196, 6.969151 ,\n",
       "        6.8565392, 6.845389 , 6.9395986, 7.0488443, 7.0647254, 6.8975677,\n",
       "        6.6062846, 6.380931 , 6.335762 , 6.5047264, 6.8638735, 7.0779147,\n",
       "        7.061319 , 6.88671  , 6.7692747, 6.7698383, 6.745297 , 6.7895265,\n",
       "        6.888773 , 7.0289087, 7.122264 , 7.1737127, 7.1263647, 6.975675 ,\n",
       "        6.7918477, 6.71065  ],\n",
       "       [5.9045415, 6.223027 , 7.0044456, 7.1401405, 7.2623553, 7.027407 ,\n",
       "        6.9622273, 6.9365125, 6.854495 , 6.7824445, 6.5897593, 6.4019904,\n",
       "        6.3160357, 6.3564224, 6.6300983, 6.8217583, 6.7822847, 6.65859  ,\n",
       "        6.5435157, 6.4742656, 6.768617 , 7.109516 , 7.1889405, 6.969713 ,\n",
       "        6.8568373, 6.845047 , 6.939159 , 7.04847  , 7.0646853, 6.897616 ,\n",
       "        6.6065273, 6.380795 , 6.335696 , 6.50512  , 6.864589 , 7.078396 ,\n",
       "        7.0618367, 6.8866625, 6.768971 , 6.7693806, 6.7457027, 6.7898226,\n",
       "        6.8887672, 7.0287766, 7.12202  , 7.1734447, 7.126464 , 6.975425 ,\n",
       "        6.791472 , 6.7103167]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.losses.sparse_categorical_crossentropy(Y_train[:2], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb396f98-89ee-4fdc-9956-406f47df2ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 50])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98c1e91b-a9f7-4102-9232-e2d00714ffbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ad7a1-6ea6-49c5-a01a-a714afcee420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
