{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943be8a2-b2a4-4d81-b245-5632d830402e",
   "metadata": {},
   "source": [
    "## Markov Decision Process and Q-value iteration\n",
    "\n",
    "## MDP specified by: \n",
    " * ### state and action spaces\n",
    " * ### state-action-state' transition probabilities (0 indicates no link in Markov chain)\n",
    " * ### state-action-state' rewards\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8a849-c30c-4bfd-bfac-c42ff2344d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1188311a-d507-4553-beb7-c12db1da670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ \n",
    "# shape=[s, a, s']\n",
    "# So dims #S * #A * #S\n",
    "[[0.7, 0.3, 0.0], \n",
    " [1.0, 0.0, 0.0], \n",
    " [0.8, 0.2, 0.0]],\n",
    "    \n",
    " [[0.0, 1.0, 0.0], \n",
    "  None, \n",
    "  [0.0, 0.0, 1.0]],\n",
    "\n",
    " [None, \n",
    "  [0.8, 0.1, 0.1], \n",
    "  None]\n",
    "]\n",
    "\n",
    "rewards = [ # shape=[s, a, s']\n",
    "[[+10, 0, 0], # Self loop: +10 for selecting action 0 in state 0 and returning to state 0\n",
    " [0, 0, 0], \n",
    " [0, 0, 0]],\n",
    "\n",
    "[[0, 0, 0], \n",
    " [0, 0, 0], \n",
    " [0, 0, -50]],\n",
    "\n",
    "[[0, 0, 0], \n",
    " [+40, 0, 0], \n",
    " [0, 0, 0]]\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500210c-8352-4f94-91c1-b3c95ed1ba91",
   "metadata": {},
   "source": [
    "## The set of actions is fixed and the same for each state, so there is a global actions enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32d3c3c-a2d1-427b-bf9f-1543e819f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [ # shape = [s, a] \n",
    " [0, 1, 2], \n",
    " [0, 2], \n",
    " [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab8653-e409-4bc0-82ad-ba565e83aec1",
   "metadata": {},
   "source": [
    "# Tabular Value Estimation: Q-value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb64f81-3916-472e-9bac-393d06293eb4",
   "metadata": {},
   "source": [
    "## Initialize Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25edc89b-4c5d-4597-b7e6-8e91bdb7fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc992e58-c2ca-4f03-a9f8-83fb4b590bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0 # Smart indexing for assigning values at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f59307c-2f1b-4e8b-a1ce-c521066bd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311563fc-edd5-4be6-a297-b3b0f62beb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(50):\n",
    "    Q_prev  = Q_values.copy()\n",
    "    for s in range(3): # 3 states in MDP\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "                for sp in range(3)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0b971a-d63c-4ff8-b719-a9cba5ea77b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ba248-bf15-465b-98c5-b0e2f7988929",
   "metadata": {},
   "source": [
    "## Optimal policy: greedy policy over Q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f16266eb-e350-4a2a-84f3-1c265c5e519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15810458-02a4-432c-8831-cc56c89b51f9",
   "metadata": {},
   "source": [
    "# Tabular Q-learning: TD for Q values, i.e. action-value estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f76895-247b-4f28-9ccb-258da2ceb8e1",
   "metadata": {},
   "source": [
    "## Q-learning is an off-policy learning, learned Q_values table is in principle not used for actions selection, but can be used in e.g. $\\epsilon$-greedy policy. In any case data samples are not generated by the same object that is being learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44648af9-f3c4-4615-b38c-463009511998",
   "metadata": {},
   "source": [
    "## Sort of brute force learning or bare statistics: learning happens even with random policy if the whole space is explored\n",
    "## Every *state-action-state'* transiton should be tried at least once to know the rewards (if rewarding is deterministic)\n",
    "## Every *state-action* choice should be tried many times to estimate *state-action(-state')* transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "760264ed-68c6-45fc-b5b4-1dd59572be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function performs action and performs state change based on \n",
    "# prescribed states transition probabilities\n",
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a232dd-dd8f-4a88-9b04-78f98d3ab1e5",
   "metadata": {},
   "source": [
    "## Random exploratory policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55767867-72a4-4d71-bb9d-5035095d045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e293d-a0db-4f57-9976-4d21b775dda3",
   "metadata": {},
   "source": [
    "## Initialize Q-values and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296eaf69-3541-4818-8b5a-5b7ae22e1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c70d701-0de5-421e-9bbb-605e4503fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05 # Initial learning rate\n",
    "decay = 0.005 # LR decay\n",
    "gamma = 0.90 # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3332f5-83ca-441b-99d0-e6bc2f535d0d",
   "metadata": {},
   "source": [
    "## General SARS paradigm like in Barto Sutton\n",
    "\n",
    "## Note: max in Q_values is in fact a trace of greedy policy selecting values based on best possible action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06922455-e38d-46d4-9473-c55ff7fcf3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 32109.77it/s]\n"
     ]
    }
   ],
   "source": [
    "state = 0 # Initial state\n",
    "\n",
    "for iteration in tqdm(range(10_000)):\n",
    "    # Take action\n",
    "    action = exploration_policy(state)\n",
    "    # Observe Reward, Next State\n",
    "    next_state, reward = step(state, action)\n",
    "    # Greedy policy next state value for max in Q-learning\n",
    "    next_value = Q_values[next_state].max()\n",
    "    # LR schedule - mandatory for SGD-like convergence\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    # Q-learning iteration step\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    # State progression\n",
    "    state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3361b86-9d2d-4e4f-b6f7-63468fd86e2d",
   "metadata": {},
   "source": [
    "## One can enhance exploration policy by counting how many times actions were selected in states and introducing curiosity with exploration function \n",
    "## In this function actions more often tried are assigned less extra value\n",
    "## In infinite exploration limit cuirosity decays to zero and Q_values converge to pure values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7941672-7c9c-40fa-ba3e-6dd5f33997e2",
   "metadata": {},
   "source": [
    "$$ Q(s, a) \\underset{\\alpha}\\leftarrow f(Q(s', a'), N(s', a'))$$\n",
    "$$f(Q, N) = Q + \\frac{\\kappa}{1+N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2321-20cc-41ec-9e72-19b03a339cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
