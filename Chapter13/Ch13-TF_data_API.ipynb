{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fafc42-0d51-45ac-9cac-215497315824",
   "metadata": {},
   "source": [
    "## TensorFlow Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc28b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2630d",
   "metadata": {},
   "source": [
    "## tf.data API loads by default:\n",
    "* ### text data\n",
    "* ### binary data of fixed size\n",
    "* ### binary data of varying size in TFRecord format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381fd4f-eab1-4cf4-92b9-094ef7a8f031",
   "metadata": {},
   "source": [
    "### TFRecord: usually contains protobuffers - open source binary format but also allows reading from SQL databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cef77-48ed-49bc-99a1-2908b0553ce3",
   "metadata": {},
   "source": [
    "### Crucially TFRecord offers extensions for reading from other sources like BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e6d42-536f-452b-a907-05bad2d415e5",
   "metadata": {},
   "source": [
    "### Keras has preprocessing layers that allow to avoid training serving skew by ingesting raw data and embedding preprocessing in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99780daf",
   "metadata": {},
   "source": [
    "### Creatig dataset from raw data\n",
    "\n",
    "### tf.data.Dataset.from_tensor_slices(<tf.Tensor>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d1b33",
   "metadata": {},
   "source": [
    "Data read by ingesting slices along the first dimension of the input tensor, so in this case the dataset will contin 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644a93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54586cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46468e1f-bcf7-432c-b0d6-b61384409543",
   "metadata": {},
   "source": [
    "## Iteration over the dataset is easy but the API is streaming, so no slicing or indexing support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702295ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8965fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856a5569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ([1, 2, 3], [4, 5, 6]), 'b': [7, 8, 9]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c231dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also available:\n",
    "# from_generator()\n",
    "# from_tensors()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd2dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2cadfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99e3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = X.repeat(3).batch(7, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772a5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a3ab0-7578-4ffa-8c0e-719b0a109d6a",
   "metadata": {},
   "source": [
    "### map() is used for data transformations, like  preprocessing \n",
    "### Can be run in parallel on many threads or scale automatically by setting tf.data.AUTOTUNE\n",
    "### Function in map must be convertible to tf.function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020bdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2, num_parallel_calls=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ccded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00341f68-21e3-42f5-80b9-8c4580dceaec",
   "metadata": {},
   "source": [
    "### Conditional filtering with a function computed on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55146ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7130fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsprint(ds):\n",
    "    for item in ds:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8f36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dsprint(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a61bd2-a5ac-4e59-9f1d-db2dce34ee58",
   "metadata": {},
   "source": [
    "### To inspect a limited number of batches from the dataset take returns a new dataset with the specified number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d73ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dsprint(dataset.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b384396",
   "metadata": {},
   "source": [
    "## Dataset API:\n",
    " * ### repeat()\n",
    " * ### batch()\n",
    " * ### shuffle()\n",
    " * ### map(<tf.function>, num_parallel_calls=tf.data.AUTOTUNE)\n",
    " * ### filter(<tf.function>)\n",
    " * ### interleave(<tf.function, cycle_length=3, num_parallel_calls=tf.data.AUTOTUNE) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229fc4a-c37f-4926-8076-aafd387b719d",
   "metadata": {},
   "source": [
    "### shuffle() randomizes large datasets with auxiliary buffer\n",
    "### It creates a new dataset and returns random samples from its buffer which is refilled with fresh samples from the original big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3033effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3630360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dsprint(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b36bb7-77a0-4c2c-ac7b-06465c7fdf85",
   "metadata": {},
   "source": [
    "## Loading dataset from CSV files and interleaving them with random shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f95302",
   "metadata": {},
   "source": [
    "## Dataset type: tf.data.TextLineDataset(<files_paths>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e417f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = './datasets/housing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8b319d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = os.listdir(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed06fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = [filepaths + train_filepaths[i] for i in range(len(train_filepaths))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7584113-25c8-472d-9de3-e3b445cd6217",
   "metadata": {},
   "source": [
    "## list_files() reads shuffled files list so this dataset will contain files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths,\n",
    "                                              seed=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9592634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'./datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/housing/my_train_03.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# This dataset contain bytestrings\n",
    "dsprint(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61bda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleaving files contents and skipping the\n",
    "# first line (the header)\n",
    "\n",
    "n_reads=3\n",
    "    # Interleave will read cycle_length files (i.e. cycle_length elements from the\n",
    "    # filepath_dataset dataset), so from this many files lines will be\n",
    "    # put together and upon shuffling will constitute a new dataset\n",
    "    # with each line being an element of the final returned dataset\n",
    "dataset = filepath_dataset.interleave(\n",
    "    # Every line of each loaded file will be a separate dataset element\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_reads,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678fa4b-9f40-4de2-b427-e1ce80891ea0",
   "metadata": {},
   "source": [
    "## Each instance of the text dataset samples is returned as a tensor \n",
    "## Strings are atomic so their sizes are not represented in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d859e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'10,2,30,40,50', shape=(), dtype=string)\n",
      "tf.Tensor(b'1,2,3,4,5', shape=(), dtype=string)\n",
      "tf.Tensor(b'100,200,300,400,500', shape=(), dtype=string)\n",
      "tf.Tensor(b'60,70,80,90,100', shape=(), dtype=string)\n",
      "tf.Tensor(b'6,7,8,9,10', shape=(), dtype=string)\n",
      "tf.Tensor(b'600,700,800,900,1000', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dsprint(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47b490",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd70f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have preprocessed mean, std of \n",
    "# each feature column in the data\n",
    "# sciki standard scaler can provide this\n",
    "X_mean, X_std = [1.,1.,1.,1.], [1.5,1.5,1.5,1.5]\n",
    "\n",
    "# Data field width in the csv:\n",
    "n_inputs = 4 #+1 for the 'target' played by the last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d53d0331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.] * n_inputs + [tf.constant([], dtype=tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d81a41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line):\n",
    "    # Default value for each column in the csv line\n",
    "    # for feature columns, which can be missing\n",
    "    # it defaults to zero float\n",
    "    # The target however must be present so default value specification is absent\n",
    "    # and only type is provided. Exception will raise\n",
    "    # on missing target in the csv line [-1] position\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47fa1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x- X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00b84338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 66.     , 132.66667, 199.33333, 266.     ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([500.], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'100,200,300,400,500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b372a7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tensor has rank 1\n",
    "# rank is the len of shape\n",
    "len((4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd52152f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar is rank 0\n",
    "len(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5649f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_reads=5, n_read_threads=None,\n",
    "                      n_parse_threads=5, shuffle_buffer_size=10,\n",
    "                      seed=42, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                cycle_length=n_reads,\n",
    "                                num_parallel_calls=n_read_threads)\n",
    "    # For small datasets - cache() to store all data in VRAM\n",
    "    # cache() after loading and preprocessing but before shuffling, batching and prefetching\n",
    "    # ensures the preprocessed data will be stored in RAM\n",
    "    # but for each epoch the randomization will be done anew\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)#.cache()\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70d233d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/housing/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64575714",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = csv_reader_dataset(train_filepaths, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f9a2143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 3.3333333,  4.       ,  4.6666665,  5.3333335],\n",
      "       [ 6.       ,  0.6666667, 19.333334 , 26.       ]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[10.],\n",
      "       [50.]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 66.     , 132.66667, 199.33333, 266.     ],\n",
      "       [399.33334, 466.     , 532.6667 , 599.3333 ]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[ 500.],\n",
      "       [1000.]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[39.333332 , 46.       , 52.666668 , 59.333332 ],\n",
      "       [ 0.       ,  0.6666667,  1.3333334,  2.       ]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[100.],\n",
      "       [  5.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "dsprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7b72b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "A = dataset.shard(num_shards=3,index=0)\n",
    "B = dataset.shard(num_shards=3,index=1)\n",
    "C = dataset.shard(num_shards=3,index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c565a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dsprint(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9f21b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 6, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(A.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1dcc676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 7]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(B.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14c46010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(C.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7efc8",
   "metadata": {},
   "source": [
    "### Windowing sequencial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6eebeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window here chops the whole sequence into smaller datasets containing \n",
    "# individual original sequence's elements as samples\n",
    "ds = tf.data.Dataset.range(30).window(5, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afc4ea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "\n",
      "\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "\n",
      "\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 14:03:03.808811: W tensorflow/core/framework/dataset.cc:959] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "for sample in ds.take(3):\n",
    "    print('\\n')\n",
    "    for x in sample:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9750-a61d-4403-a388-e5da70b01aa0",
   "metadata": {},
   "source": [
    "### Here window size must equal to the windowing size to form such window-sized samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9473674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.flat_map(lambda wind_ds: wind_ds.batch(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c6872e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int64)\n",
      "tf.Tensor([2 3 4 5 6], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for sample in ds.take(3):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c36fb4-fa7d-4257-b848-1410e1e4bd3a",
   "metadata": {},
   "source": [
    "## Custom training loop with tf.function and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9045086d-cd53-4131-b6fc-71c047faad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ff0dbe9-4b1d-4cbe-8d2d-762337092bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  3.3333333   4.          4.6666665   5.3333335]\n",
      " [  6.          0.6666667  19.333334   26.       ]\n",
      " [ 66.        132.66667   199.33333   266.       ]\n",
      " [399.33334   466.        532.6667    599.3333   ]\n",
      " [ 39.333332   46.         52.666668   59.333332 ]\n",
      " [  0.          0.6666667   1.3333334   2.       ]], shape=(6, 4), dtype=float32) tf.Tensor(\n",
      "[[  10.]\n",
      " [  50.]\n",
      " [ 500.]\n",
      " [1000.]\n",
      " [ 100.]\n",
      " [   5.]], shape=(6, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb26ee26-adb6-42c0-8231-650d6985c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=[4], activation='relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eed3aff9-6d21-43e8-8323-94e84afcd08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 14:03:05.694878: I external/local_xla/xla/service/service.cc:168] XLA service 0x75ee7c7fc870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-28 14:03:05.694899: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-02-28 14:03:05.730013: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709125385.793854  116436 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            # Main model loss\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            # Efficiently adding regularizers and other losses\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\rEpoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6c3ca-43d3-4105-add5-489170e93066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
